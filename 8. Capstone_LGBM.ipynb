{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "# Capstone\n",
    "# Project: To find an Optimum Machine Learning Model for Human Activity        Recognition with Smartphone Sensor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Capstone for MLND.\n",
    "Our purpose is to recognize human activity based on the sensor data from smartphones and to find an optimum model with can predict these activities with high degree of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Necessary Libraries\n",
    "\n",
    "import time, pickle\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "\n",
    "# Use pickle to open stored objects.\n",
    "with open('data.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "with open('X_train.pickle','rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('X_test.pickle','rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open('y_train.pickle','rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('y_test.pickle','rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "with open('y_true.pickle','rb') as f:\n",
    "    y_true = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Classifier:\n",
    "\n",
    "Now let us use LightGBM Classifier for our task. At first we will use the classifier with the default parameter values and then tune its parameters to obtain better accuracy and logloss Score. Let's see if how good a score can we obtain with LGBM Classifier :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.1,\n",
       "        max_bin=255, max_depth=-1, min_child_samples=10,\n",
       "        min_child_weight=5, min_split_gain=0.0, n_estimators=10, n_jobs=-1,\n",
       "        num_leaves=31, objective=None, random_state=0, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=50000, subsample_freq=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import LightGBM Classifier for our Task\n",
    "from lightgbm import LGBMClassifier\n",
    "# Define the classifier\n",
    "clf_lgbm = LGBMClassifier()\n",
    "clf_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for LGBM Classifier is: 0.96957928802589\n",
      "F1 Score for LGBM Classifier is: 0.9695155916317623\n",
      "Runtime is 406.51 seconds\n",
      "Log Loss Score is: 0.595676708696094\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "clf_lgbm.fit(X_train, y_train)\n",
    "## make predictions\n",
    "preds = clf_lgbm.predict(X_test)\n",
    "end = time.clock()\n",
    "y_pred = clf_lgbm.predict_proba(X_test)\n",
    "print('Accuracy Score for LGBM Classifier is: {}'.format(accuracy_score(y_test, preds)))\n",
    "print('F1 Score for LGBM Classifier is: {}'.format(f1_score(y_test, preds, average = 'weighted')))\n",
    "print('Runtime is {} seconds'.format(end-start))\n",
    "print('Log Loss Score is: {}'.format(log_loss(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### GridSearchCV for tuning Parameters:\n",
    "Now we will select _'objective'_ as _multiclass_ and _'num_class'_ as 6 and then do parameter tuning for 'num_ierations' and 'learning_rate' followed by the following parameters:\n",
    "* num_leaves\n",
    "* min_data_in_leaf\n",
    "* max_depth\n",
    "\n",
    "_(Reference: http://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.5, 'num_iterations': 100}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets set learning rate as 0.1 and see what optimum value we get for num_iterations. Our objective is to obtain a num_iterations value closer to 100 \n",
    "# and then for final score evaluation we will increase this consequently decreasing learnig rate. \n",
    "params = {'num_iterations' : [i*10 for i in range(6, 15, 2)],\n",
    "          'learning_rate' : [0.5]}\n",
    "GSCV = GridSearchCV(clf_lgbm, params, verbose = True, n_jobs = -1)\n",
    "GSCV.fit(X_train, y_train)\n",
    "GSCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of  12 | elapsed:   44.3s remaining:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'num_leaves': 8}, 0.98807046747121652)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets set num_iterations= 100 and learning_rate=0.5 and move on with tuning the other parameters\n",
    "clf_lgbm = LGBMClassifier(objective = 'multiclass', num_class = 6, learning_rate = 0.5, num_iterations = 100)\n",
    "\n",
    "#GridSearch for 'num_leaves'\n",
    "params = {'num_leaves' : [2**i for i in range(2, 6)]}\n",
    "GSCV = GridSearchCV(clf_lgbm, params, verbose = True, n_jobs = -1)\n",
    "GSCV.fit(X_train, y_train)\n",
    "GSCV.best_params_, GSCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of  12 | elapsed:   47.5s remaining:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   49.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'min_data_in_leaf': 300}, 0.98820918296573723)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lgbm = LGBMClassifier(boosting_type='gbdt', objective = 'multiclass', num_class = 6, learning_rate = 0.5, num_iterations = 100, num_leaves = 8)\n",
    "\n",
    "#GridSearch for 'num_leaves'\n",
    "params = {'min_data_in_leaf' : [i*100 for i in range(1, 5)]}\n",
    "GSCV = GridSearchCV(clf_lgbm, params, verbose = True, n_jobs = -1)\n",
    "GSCV.fit(X_train, y_train)\n",
    "GSCV.best_params_, GSCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'min_data_in_leaf': 220}, 0.98848661395477877)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets tune 'num_leaves' one more time with range from 200 to 400 with step of 10\n",
    "# Grid Search CV\n",
    "params = {'min_data_in_leaf' : [i for i in range(200, 400, 10)]}\n",
    "GSCV = GridSearchCV(clf_lgbm, params, verbose = True)\n",
    "GSCV.fit(X_train, y_train)\n",
    "GSCV.best_params_, GSCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'min_data_in_leaf': 229}, 0.98890276043834091)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one last time for 'num_leaves' \n",
    "params = {'min_data_in_leaf' : [i for i in range(210, 230)]}\n",
    "GSCV = GridSearchCV(clf_lgbm, params, verbose = True)\n",
    "GSCV.fit(X_train, y_train)\n",
    "GSCV.best_params_, GSCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'max_depth': 7}, 0.98890276043834091)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search for 'max_depth'\n",
    "params = {'max_depth' : [i for i in range(6, 10)]}\n",
    "GSCV = GridSearchCV(clf_lgbm, params, verbose = True)\n",
    "GSCV.fit(X_train, y_train)\n",
    "GSCV.best_params_, GSCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for LGBM Classifier is: 0.991262135922\n",
      "F1 Score for LGBM Classifier is: 0.991257859265\n",
      "Runtime is 264.508288 seconds\n",
      "Log Loss Score is: 0.0247173917944\n"
     ]
    }
   ],
   "source": [
    "# Pass tuned params to the classifier. Using seed for reproducible results.\n",
    "# Decrease learning rate and increase num_iterations correspondingly. num_iteration from 100 to 1000 and learning_rate from 0.5 to 0.05)\n",
    "clf_lgbm = LGBMClassifier(boosting_type='gbdt', objective = 'multiclass', num_class = 6,\n",
    "                          learning_rate = 0.05, num_iterations = 1000, num_leaves = 8, \n",
    "                          min_data_in_leaf = 229, max_depth = 7, seed = 31)\n",
    "\n",
    "# Fit to the training data\n",
    "start = time.clock()\n",
    "clf_lgbm.fit(X_train, y_train)\n",
    "## make predictions\n",
    "preds = clf_lgbm.predict(X_test)\n",
    "end = time.clock()\n",
    "y_pred = clf_lgbm.predict_proba(X_test)\n",
    "print('Accuracy Score for LGBM Classifier is: {}'.format(accuracy_score(y_test, preds)))\n",
    "print('F1 Score for LGBM Classifier is: {}'.format(f1_score(y_test, preds, average = 'weighted')))\n",
    "print('Runtime is {} seconds'.format(end-start))\n",
    "print('Log Loss Score is: {}'.format(log_loss(y_true, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
