{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "# Capstone\n",
    "# Project: To find an Optimum Machine Learning Model for Human Activity        Recognition with Smartphone Sensor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Capstone for MLND.\n",
    "Our purpose is to recognize human activity based on the sensor data from smartphones and to find an optimum model with can predict these activities with high degree of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Necessary Libraries\n",
    "\n",
    "import time, pickle\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use pickle to open stored objects.\n",
    "with open('data.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "with open('X_train.pickle','rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('X_test.pickle','rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open('y_train.pickle','rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('y_test.pickle','rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "with open('y_true.pickle','rb') as f:\n",
    "    y_true = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction without Pre-proceesing:\n",
    "\n",
    "Let us fit our dataset into a \"Naive Bayes\" model before pre-proceesing to see if the pre-proceesing step would at all increase our accuracy or runtime. Lets store the accuracy and runtime as raw_acc and raw_time and after pre-processing the dataset we will compare both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Gaussian Naive Bayes Model, accuracy score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test set:\n",
    "raw_data = data  # Store the dataframe as raw_data\n",
    "label = raw_data['Activity']  # Store trget feture as 'label'\n",
    "raw_data = raw_data.drop(['Activity'], axis = 1)  # Drop the target feature (562 Columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_data, label, test_size = 0.3, random_state = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for NB on the unprocessed dataset is: 0.690614886731\n",
      "Runtime required: 0.193971\n"
     ]
    }
   ],
   "source": [
    "# Define a classifier and use it to fit and predict the data\n",
    "clf_nb= GaussianNB()\n",
    "start = time.clock() # Store the start time\n",
    "clf_nb.fit(X_train, y_train) # Fit the NB classifier\n",
    "pred_nb = clf_nb.predict(X_test) # Use the fitted NB classifier to predict on the Test set\n",
    "end = time.clock() # Store the end execution time for the model\n",
    "runtime_raw = end-start  # Save the runtime\n",
    "raw_acc = accuracy_score(y_test, pred_nb) # store the model aacuracy score as raw_accuracy\n",
    "print('Accuracy Score for NB on the unprocessed dataset is: {}'.format(raw_acc)) #Print out accuracy score\n",
    "print('Runtime required: {}'.format(runtime_raw))  # Print out runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Processed Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for NB is: 0.690614886731\n",
      "Runtime is 0.167511\n"
     ]
    }
   ],
   "source": [
    "# predict accuracy with NB using the processed data\n",
    "start = time.clock()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "end = time.clock()\n",
    "runtime = end-start\n",
    "acc = accuracy_score(y_test, pred_nb)\n",
    "print('Accuracy Score for NB is: {}'.format(acc))\n",
    "print('Runtime is {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for NB with unprocessed data is: 0.690614886731\n",
      "Runtime is 0.193971\n",
      "\n",
      "Accuracy Score for NB with Processed data is: 0.690614886731\n",
      "Runtime is 0.167511\n"
     ]
    }
   ],
   "source": [
    "# Compare the accuracy and runtimes in both unprocessed and procesed cases:\n",
    "print('Accuracy Score for NB with unprocessed data is: {}'.format(raw_acc))\n",
    "print('Runtime is {}'.format(runtime_raw))\n",
    "print('')\n",
    "print('Accuracy Score for NB with Processed data is: {}'.format(acc))\n",
    "print('Runtime is {}'.format(runtime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "** Result:**\n",
    "As we can see from our above comparision, accuracy score in case of both processed and unprocessed is same.\n",
    "\n",
    "However, for the same accuracy, the raw Naive Bayes model is taking more runtime than the model with the pre-processed dataset. So, for all the further models, we will consider the preprocessed dataset with 561 Features as our training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
